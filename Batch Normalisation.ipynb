{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb50d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Explanation of Batch Normalization:\n",
    "Batch normalization is a technique used in artificial neural networks to improve the training process and overall performance. It involves normalizing the inputs of each layer in a mini-batch before passing them through the activation function. The normalized values are then scaled and shifted using learnable parameters, allowing the network to adapt and optimize during training.\n",
    "\n",
    "2.Benefits of Batch Normalization:\n",
    "Stability during Training: Batch normalization helps in stabilizing and accelerating the training process by reducing internal covariate shift, making each layer's input distribution more consistent.\n",
    "Regularization Effect: It acts as a form of regularization, reducing the reliance on dropout and other regularization techniques, which can be beneficial in preventing overfitting.\n",
    "Faster Convergence: Batch normalization can lead to faster convergence during training, allowing the network to reach a good solution more quickly.\n",
    "\n",
    "3.Working Principle of Batch Normalization:\n",
    "Batch normalization involves two main steps: normalization and transformation.\n",
    "Normalization Step: Calculate the mean and standard deviation of the input values within a mini-batch. Normalize the values by subtracting the mean and dividing by the standard deviation.\n",
    "Learnable Parameters: Introduce learnable parameters (gamma and beta) for each feature to scale and shift the normalized values. These parameters allow the model to adapt and learn the most suitable scale and shift for each feature during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Dataset and Preprocessing:\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((-1, 28 * 28)).astype('float32') / 255.0\n",
    "x_test = x_test.reshape((-1, 28 * 28)).astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "2.Implement Feedforward Neural Network:\n",
    "Let's create a simple feedforward neural network using TensorFlow. The network will have two hidden layers with ReLU activation.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the model without batch normalization\n",
    "def create_model_without_bn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Define the model with batch normalization\n",
    "def create_model_with_bn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=784, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "3.\n",
    "Train the Models:\n",
    "Now, let's train the models without and with batch normalization and compare their performance.\n",
    "# Train the model without batch normalization\n",
    "model_without_bn = create_model_without_bn()\n",
    "model_without_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_without_bn = model_without_bn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "4.\n",
    "# Train the model with batch normalization\n",
    "model_with_bn = create_model_with_bn()\n",
    "model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_with_bn = model_with_bn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "5.Compare Performance:\n",
    "Compare training and validation accuracy and loss for both models.\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_without_bn.history['accuracy'], label='Without BN')\n",
    "plt.plot(history_with_bn.history['accuracy'], label='With BN')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_without_bn.history['loss'], label='Without BN')\n",
    "plt.plot(history_with_bn.history['loss'], label='With BN')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "6.We observe that the model with batch normalization achieves better training accuracy, converges faster, and has a lower training loss compared to the model without batch normalization. Batch normalization helps stabilize and accelerate training by normalizing inputs, mitigating issues like vanishing gradients and allowing the network to learn more effectively. It also acts as a form of regularization, which may improve generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Train the model with batch normalization and different batch sizes\n",
    "    model_with_bn = create_model_with_bn()\n",
    "    model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history_with_bn = model_with_bn.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\n",
    "    \n",
    "    # Plot training and validation accuracy for each batch size\n",
    "    plt.plot(history_with_bn.history['accuracy'], label=f'Batch Size {batch_size}')\n",
    "\n",
    "plt.title('Training Accuracy with Batch Normalization and Different Batch Sizes')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "2.Advantages of Batch Normalization:\n",
    "\n",
    "Stabilized Training: Batch normalization helps in stabilizing the training process by reducing internal covariate shift, making each layer's input distribution more consistent across batches.\n",
    "Faster Convergence: It accelerates the training process, allowing the network to converge faster to a good solution. This is particularly beneficial for deep networks.\n",
    "Regularization: Batch normalization acts as a form of regularization, reducing the need for other regularization techniques such as dropout. It can improve the model's generalization to unseen data.\n",
    "Mitigates Vanishing Gradient: Batch normalization mitigates the vanishing gradient problem, allowing gradients to flow more freely during backpropagation.\n",
    "Potential Limitations:\n",
    "Computational Overhead: Batch normalization introduces additional computations during both training and inference, which may impact the overall computational efficiency.\n",
    "Dependency on Batch Size: The effectiveness of batch normalization can be dependent on the choice of batch size. Very small batch sizes might not provide sufficient statistics for normalization, leading to less effective performance.\n",
    "Not Suitable for Some Architectures: Batch normalization may not be suitable for certain types of architectures, such as recurrent neural networks (RNNs), where the order of inputs is crucial.\n",
    "Sensitivity to Learning Rate: Batch normalization can be sensitive to the choice of learning rates. It may require fine-tuning of hyperparameters for optimal performance.\n",
    "In conclusion, while batch normalization offers significant benefits in improving the training dynamics and performance of neural networks, practitioners should be mindful of its potential limitations and adjust hyperparameters accordingly for optimal results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
